{"cells":[{"cell_type":"markdown","metadata":{"id":"f8-UD2KVpV_q"},"source":["**Project Objective: Developing Gesture Recognition for Smart TV Control**\n","\n","As a data scientist at a leading home electronics company specializing in cutting-edge smart television manufacturing, the goal is to enhance the user experience by introducing a novel feature: gesture recognition. This feature aims to enable users to control their smart TVs through intuitive hand gestures, eliminating the need for a traditional remote control. The project's focus is on recognizing five distinct gestures – Thumbs Up, Thumbs Down, Left Swipe, Right Swipe, and Stop – each corresponding to specific actions such as volume adjustment, playback control, and pausing.\n","\n","**Challenges and Objectives:**\n","\n","**Generator Enhancement:**\n","The first challenge is to develop a reliable generator that can process batches of video sequences seamlessly. This involves tasks like cropping, resizing, and normalization of frames to ensure compatibility with the model's input requirements. The generator's performance is crucial for generating training data that feeds into the model effectively.\n","\n","**Model Development:**\n","The primary objective is to design a model capable of training without errors while achieving a balance between parameter efficiency (for reduced inference time) and accuracy. To ensure gradual progress, the initial training will be conducted on a limited dataset before scaling up.\n","\n","**Project Phases:**\n","\n","**1. Generator Enhancement:**\n","The project kicks off by refining the data generator. This component should seamlessly process batches of video sequences while performing necessary pre-processing steps, such as cropping to relevant regions of interest, resizing for consistent input dimensions, and normalization for optimal convergence during training. This stage aims to provide the model with well-prepared data to learn from.\n","\n","**2. Initial Model Training:**\n","Beginning with a solid data generator, the focus shifts to designing an initial model architecture. The choice of base model is crucial. A model with a balance between complexity and performance, such as a Convolutional Neural Network (CNN), is selected as the starting point. This decision is influenced by the CNN's established capabilities in image-related tasks and its compatibility with the sequential nature of video frames.\n","\n","**3. Iterative Model Refinement:**\n","The model enhancement phase involves iterations and experiments to fine-tune the architecture for optimal performance. Metrics like accuracy and inference time guide these decisions. Starting with a smaller dataset allows for quicker iterations and aids in identifying the most promising directions for improvement.\n","\n","**4. Choosing the Final Model:**\n","The iterative process aims to reach a final model that strikes the ideal balance between accuracy and parameter efficiency. The write-up will detail the reasoning behind selecting the base model and the successive modifications. Each modification will be grounded in clear reasoning, supported by relevant metrics like accuracy achieved, training convergence speed, and inference time.\n","\n","**Write-up Structure:**\n","\n","1. **Introduction:**\n","   - Briefly explain the project's goal: implementing gesture recognition for smart TV control.\n","   \n","2. **Generator Enhancement:**\n","   - Discuss the importance of a robust data generator for model training.\n","   - Detail the steps taken to preprocess videos into usable training data.\n","\n","3. **Model Development:**\n","   - Justify the choice of a CNN as the base model due to its suitability for image-based tasks.\n","   \n","4. **Iterative Refinement:**\n","   - Describe the iterative process of tweaking the model to improve accuracy and efficiency.\n","   \n","5. **Final Model Selection:**\n","   - Explain the final model's architecture and modifications in detail.\n","   - Present metrics supporting the selection, including accuracy and inference time.\n","\n","6. **Conclusion:**\n","   - Summarize the journey from the initial base model to the final optimized model.\n","   - Highlight the achieved balance between gesture recognition accuracy and efficient parameter usage.\n","\n","By adhering to this comprehensive plan, the objective of developing a sophisticated gesture recognition system for smart TVs can be effectively achieved, enhancing the user experience and positioning the company at the forefront of innovation in the home electronics industry.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693191586032,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"BlLK1lDdphpr","outputId":"b2a281fb-97e6-44f0-a9ab-ce487526a1a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mon Aug 28 02:59:42 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    49W / 400W |  38979MiB / 40960MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"]}],"source":["## Checking the GPU configuration\n","\n","!nvidia-smi"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1693191586033,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"U3HTAsWYpV_v"},"outputs":[],"source":["# Importing Required Libraries\n","import numpy as np\n","import os\n","from imageio import imread\n","from skimage.transform import resize\n","import datetime\n","import os"]},{"cell_type":"markdown","metadata":{"id":"O7oBQ7S3pV_x"},"source":["We set the random seed so that the results don't vary drastically."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1693191586369,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"DFMwbV5fpV_y"},"outputs":[],"source":["# Importing Required Libraries\n","np.random.seed(30)\n","import random as rn\n","rn.seed(30)\n","from tensorflow import keras\n","import tensorflow as tf\n","tf.random.set_seed(30)\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import Conv3D\n","from tensorflow.keras.layers import ConvLSTM2D\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import GlobalAveragePooling2D\n","from tensorflow.keras.layers import GlobalAveragePooling3D\n","from tensorflow.keras.layers import GRU\n","from tensorflow.keras.layers import LSTM\n","from tensorflow.keras.layers import MaxPooling3D, MaxPooling2D\n","from tensorflow.keras.layers import TimeDistributed\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras import optimizers"]},{"cell_type":"markdown","metadata":{"id":"DvqcUA9tpV_y"},"source":["In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3212,"status":"ok","timestamp":1693191591489,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"omZr3EnwpV_z","outputId":"a5dccda3-d859-4883-b0a5-ba1957039afe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Importing Required Libraries\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from google.colab import drive\n","drive.mount('/content/drive')\n","project_folder = '/content/drive/My Drive/Gesture Recognition/Project_data/Project_data'\n","train_doc = np.random.permutation(open('/content/drive/My Drive/Gesture Recognition/Project_data/Project_data/train.csv').readlines())\n","val_doc = np.random.permutation(open('/content/drive/My Drive/Gesture Recognition/Project_data/Project_data/val.csv').readlines())\n","batch_size = 39"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1693191591489,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"hLxezZF1pV_z"},"outputs":[],"source":["#def image_specifications(nb_frames, x, y):\n","    #return [np.round(np.linspace(0, 29, nb_frames)).astype('int'), x, y]\n","\n","\n","#img_specs = image_specifications(20, 100, 100)\n","\n","# Image Specifications Generator\n","# This code defines a function to generate specifications for images with specific parameters.\n","\n","\n","def generate_image_specs(num_frames, x_coord, y_coord):\n","    \"\"\"\n","    Generate specifications for images with given parameters.\n","\n","    :param num_frames: Number of frames.\n","    :param x_coord: X-coordinate.\n","    :param y_coord: Y-coordinate.\n","    :return: Image specifications as a list.\n","    \"\"\"\n","    frame_indices = np.round(np.linspace(0, 29, num_frames)).astype('int')\n","    image_specs = [frame_indices, x_coord, y_coord]\n","    return image_specs\n","\n","# Generate image specifications\n","img_specs = generate_image_specs(20, 100, 100)\n"]},{"cell_type":"markdown","metadata":{"id":"ubpo1aPZpV_0"},"source":["## Data Generator\n","The data generator is a crucial component in the code. The provided structure outlines its overall design. Within the generator, you'll undertake image preprocessing, considering the presence of images with two different dimensions. Additionally, the generation of a video frame batch is essential. Experimentation with parameters such as `img_idx`, `y`, `z`, and normalization is required to achieve optimal accuracy."]},{"cell_type":"markdown","metadata":{"id":"suPaEbE-xOjZ"},"source":["The function get_batch_labels_and_data prepares batch data and labels by processing images, cropping, resizing, and normalizing them. The generator function uses this data generator within an infinite loop to yield batches of processed data and labels for training. The generator first shuffles the input data and then iterates over the shuffled data to provide batches of data. The remaining data points that don't form a full batch are also handled"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":345,"status":"ok","timestamp":1693191594443,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"XbXm7vQzpV_0"},"outputs":[],"source":["def get_batch_labels_and_data(source_path, t, batch, batch_size, img_specs):\n","    x, y, z = len(img_specs[0]), img_specs[1], img_specs[2]\n","    img_idx = img_specs[0]  # create a list of image numbers you want to use for a particular video\n","    batch_data = np.zeros((batch_size, x, y, z, 3))  # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n","    batch_labels = np.zeros((batch_size, 5))  # batch_labels is the one hot representation of the output\n","\n","    for folder in range(batch_size):  # iterate over the batch_size\n","        imgs = os.listdir('{0}/{1}'.format(source_path, t[folder + (batch * batch_size)].split(';')[0]))  # read all the images in the folder\n","\n","        for idx, item in enumerate(img_idx):  # Iterate iver the frames/images of a folder to read them in\n","            image = imread('{0}/{1}/{2}'.format(source_path, t[folder + (batch * batch_size)].strip().split(';')[0], imgs[item])).astype(np.float32)\n","\n","            #crop the images and resize them. Note that the images are of 2 different shape\n","            #and the conv3D will throw error if the inputs in a batch have different shapes\n","\n","            if image.shape[0] != image.shape[1]:\n","                image = image[:120, 20:140]\n","            image = resize(image, (y, z))\n","\n","            batch_data[folder, idx, :, :, 0] = image[:, :, 0] / 255.0  # normalise and feed in the image\n","            batch_data[folder, idx, :, :, 1] = image[:, :, 1] / 255.0  # normalise and feed in the image\n","            batch_data[folder, idx, :, :, 2] = image[:, :, 2] / 255.0  # normalise and feed in the image\n","\n","        batch_labels[folder, int(t[folder + (batch * batch_size)].strip().split(';')[2])] = 1\n","\n","    return batch_data, batch_labels\n","\n","\n","def generator(source_path, folder_list, batch_size, img_specs=img_specs):\n","    print(f\"Source Path: {source_path}; Batch Size: {batch_size}\")\n","    while True:\n","        t = np.random.permutation(folder_list)\n","        num_batches = int(len(folder_list) / batch_size)\n","        for batch in range(num_batches):  # we iterate over the number of batches\n","            yield get_batch_labels_and_data(source_path, t, batch, batch_size, img_specs)  # you yield the batch_data and the batch_labels, remember what does yield do\n","\n","        # write the code for the remaining data points which are left after full batches\n","        if len(folder_list) % batch_size != 0:\n","            batch_size = len(folder_list) % batch_size\n","            yield get_batch_labels_and_data(source_path, t, batch, batch_size, img_specs)  # you yield the batch_data and the batch_labels, remember what does yield do"]},{"cell_type":"markdown","metadata":{"id":"TwRKOxv0pV_1"},"source":["Please take note that in the generator, a video is represented as a tensor with dimensions (number of images, height, width, number of channels). Keep this representation in mind when designing the model architecture."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1,"status":"ok","timestamp":1693191596159,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"3EeRsc8LpV_2","outputId":"4d1f8682-b54d-425c-830d-9e4b91c731bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["# training seq: 663\n","# validation seq: 100\n","# epcohs: 20\n"]}],"source":["# initializes the current date and time, sets up paths for training and validation data, and then\n","# prints out the number of training and validation sequences as well as the number of epochs to be used in the training process.\n","curr_dt_time = datetime.datetime.now()\n","train_path = '/content/drive/My Drive/Gesture Recognition/Project_data/Project_data/train'\n","\n","val_path = '/content/drive/My Drive/Gesture Recognition/Project_data/Project_data/val'\n","num_train_sequences = len(train_doc)\n","print(f\"# training seq: {num_train_sequences}\")\n","num_val_sequences = len(val_doc)\n","print(f\"# validation seq: {num_val_sequences}\")\n","num_epochs = 20\n","print(f\"# epcohs: {num_epochs}\")"]},{"cell_type":"markdown","metadata":{"id":"Yzh5JHqGpV_3"},"source":["## Model Architecture\n","In this section, the model is constructed using various functionalities offered by Keras. It's important to utilize `Conv3D` and `MaxPooling3D` for creating a 3D convolutional model, avoiding the use of `Conv2D` and `MaxPooling2D`. If implementing a Conv2D + RNN model, be sure to incorporate `TimeDistributed`. Additionally, keep in mind that the final layer should utilize the softmax activation function.\n","\n","The network design should prioritize achieving high accuracy with minimal parameter usage to ensure compatibility with the memory constraints of the webcam."]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693191597894,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"VuhZlDEWpV_3"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693191599254,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"xJKqisr6pV_4"},"outputs":[],"source":["\n","# Define the input shape for the model\n","# The input shape represents (number of images, height, width, number of channels)\n","input_shape = (len(img_specs[0]), img_specs[1], img_specs[2], 3)"]},{"cell_type":"markdown","metadata":{"id":"q_DVAvG-pV_4"},"source":["### Model - 1:"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":362,"status":"ok","timestamp":1693191601130,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"GVjzkSebpV_4"},"outputs":[],"source":["\n","\n","# # model\n","\n","model = Sequential()\n","model.add(Conv3D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n","\n","model.add(Conv3D(63, kernel_size=3, activation='relu'))\n","model.add(MaxPooling3D(pool_size=2))\n","\n","model.add(Flatten())\n","model.add(Dense(256, activation='relu'))\n","model.add(Dense(5, activation='softmax'))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"M1-8F6yepV_5"},"source":["Number of Epochs: 20 \u003cbr /\u003e\n","Training Accuracy: 0.20 | Validation Accuracy: 0.41 \u003cbr /\u003e\n","The current model demonstrates limited learning capability. To address this, additional layers will be incorporated into the model."]},{"cell_type":"markdown","metadata":{"id":"pNMuBsqapV_5"},"source":["### Model - 2"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":348,"status":"ok","timestamp":1693191607574,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"riPh2WzgpV_5"},"outputs":[],"source":["# # model 2 -- increasing layers\n","\n","# model = Sequential()\n","\n","# model.add(Conv3D(16, (3, 3, 3), padding='same',input_shape=input_shape))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","\n","# model.add(Conv3D(16, (3, 3, 3), padding='same',input_shape=input_shape))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Conv3D(32, (3, 3, 3), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","\n","# model.add(Conv3D(32, (3, 3, 3), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Conv3D(64, (3, 3, 3), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","\n","# model.add(Conv3D(64, (3, 3, 3), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Conv3D(128, (3, 3, 3), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","\n","# model.add(Conv3D(128, (3, 3, 3), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Flatten())\n","# model.add(Dense(64, activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.25))\n","\n","# model.add(Dense(63, activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.25))\n","\n","# model.add(Dense(5, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"HwRse4KVpV_5"},"source":["Number of Epochs: 20\u003cbr /\u003e\n","Training Accuracy: 0.61 | Validation Accuracy: 0.75 \u003cbr /\u003e\n","Both the training and validation accuracies fall short of expectations. To address this, certain parameters will be reduced in an attempt to enhance model performance."]},{"cell_type":"markdown","metadata":{"id":"HMPQL3_0pV_5"},"source":["### Model - 3"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693191609533,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"pIkBe7uIpV_5"},"outputs":[],"source":["# # model 3 -- reducing parameters\n","\n","# model = Sequential()\n","\n","# model.add(Conv3D(16, (3, 3, 3), padding='same',input_shape=input_shape))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Conv3D(32, (2, 2, 2), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Conv3D(64, (2, 2, 2), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Conv3D(128, (2, 2, 2), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Flatten())\n","# model.add(Dense(64,activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.25))\n","\n","# model.add(Dense(64, activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.25))\n","\n","# model.add(Dense(5, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"x7BbXHAppV_6"},"source":["Number of Epochs: 20 \u003cbr /\u003e\n","Training Accuracy: 0.97 | Validation Accuracy: 0.83 \u003cbr /\u003e\n","The model appears to be exhibiting signs of overfitting. To mitigate this, a further reduction in the number of parameters is warranted."]},{"cell_type":"markdown","metadata":{"id":"PP40WpXwpV_6"},"source":["### Model - 4"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1693191610826,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"wQ8UicvppV_6"},"outputs":[],"source":["# # model 4 -- reducing more parameters\n","\n","# model = Sequential()\n","\n","# model.add(Conv3D(16, (3, 3, 3), padding='same',input_shape=input_shape))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Conv3D(32, (3, 3, 3), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Conv3D(64, (2, 2, 2), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Conv3D(128, (2, 2, 2), padding='same'))\n","# model.add(Activation('relu'))\n","# model.add(BatchNormalization())\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","\n","# model.add(Flatten())\n","# model.add(Dense(64, activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.25))\n","\n","# model.add(Dense(64, activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.25))\n","\n","# model.add(Dense(5, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"cz6c4l-dpV_6"},"source":["Number of Epochs: 20 \u003cbr /\u003e\n","Training Accuracy: 0.17 | Validation Accuracy: 0.5 \u003cbr /\u003e\n","The extensive reduction in parameters has resulted in a severely underfitting model. \u003cbr /\u003e\n","To address this, the number of epochs will be increased from 20 to 40. Additionally, a new model will be designed, and the Dropout layers will be removed."]},{"cell_type":"markdown","metadata":{"id":"x-E-aBI3pV_6"},"source":["### Model - 5"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1693191611615,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"viPqStpppV_6"},"outputs":[],"source":["# # model - 5 # Increasing parameters\n","\n","# model = Sequential()\n","\n","# model.add(Conv3D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n","# model.add(Conv3D(64, kernel_size=3, activation='relu'))\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","# model.add(BatchNormalization())\n","\n","# model.add(Conv3D(128, kernel_size=3, activation='relu'))\n","# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n","# model.add(BatchNormalization())\n","\n","# model.add(Conv3D(256, kernel_size=(1, 3, 3), activation='relu'))\n","# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n","# model.add(BatchNormalization())\n","\n","# model.add(Conv3D(512, kernel_size=(1, 3, 3), activation='relu'))\n","# model.add(Conv3D(512, kernel_size=(1, 3, 3), activation='relu'))\n","# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n","# model.add(BatchNormalization())\n","\n","# model.add(Flatten())\n","# model.add(Dense(512, activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dense(5, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"f65AWiJMpV_6"},"source":["Number of Epochs: 40\u003cbr /\u003e\n","Training Accuracy: 1.00 | Validation Accuracy: 0.91 \u003cbr /\u003e\n","The evident overfitting of the model is a concern. To address this, a reduction in the number of parameters will be implemented, along with reintroducing Dropout layers."]},{"cell_type":"markdown","metadata":{"id":"HeLENEcjpV_7"},"source":["### Model - 6"]},{"cell_type":"code","execution_count":30,"metadata":{"executionInfo":{"elapsed":0,"status":"ok","timestamp":1693191611987,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"lPRg9-TNpV_7"},"outputs":[],"source":["# # model - 6 Reducing the parameter count and reintroducing Dropout layers.\n","\n","# model = Sequential()\n","\n","# model.add(Conv3D(16, (5, 5, 5), activation='relu', input_shape=input_shape))\n","# model.add(MaxPooling3D((2, 2, 2), padding='same'))\n","# model.add(BatchNormalization())\n","\n","# model.add(Conv3D(32, (3, 3, 3), activation='relu'))\n","# model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n","# model.add(BatchNormalization())\n","\n","# model.add(Conv3D(64, (3, 3, 3), activation='relu'))\n","# model.add(MaxPooling3D(pool_size=(1, 2, 2), padding='same'))\n","# model.add(BatchNormalization())\n","\n","# model.add(Flatten())\n","# model.add(Dense(128, activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.2))\n","\n","# model.add(Dense(64, activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.2))\n","\n","# model.add(Dense(5, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"5XZ8R9yxpV_7"},"source":["Number of Epochs: 40 \u003cbr /\u003e\n","Training Accuracy: 0.99 | Validation Accuracy: 1.00 \u003cbr /\u003e\n","Despite a substantial rise in validation accuracy, the model continues to exhibit signs of overfitting. \u003cbr /\u003e\n","To address this, an approach involving the introduction of additional Dropout layers and transitioning from Flatten to GlobalAveragePooling3D will be pursued."]},{"cell_type":"markdown","metadata":{"id":"L90Yf3KZpV_7"},"source":["### Model - 7"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1693191612381,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"OKoQzqBDpV_8"},"outputs":[],"source":["# # model - 7 Elevating the count of Dropout layers and substituting Flatten with GlobalAveragePooling3D.\n","\n","# model = Sequential()\n","\n","# model.add(Conv3D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n","# model.add(Conv3D(64, kernel_size=3, activation='relu'))\n","# model.add(MaxPooling3D(pool_size=(2, 2, 2)))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.2))\n","\n","# model.add(Conv3D(128, kernel_size=3, activation='relu'))\n","# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.2))\n","\n","# model.add(Conv3D(256, kernel_size=(1, 3, 3), activation='relu'))\n","# model.add(MaxPooling3D(pool_size=(1, 2, 2)))\n","# model.add(BatchNormalization())\n","# model.add(Dropout(0.2))\n","\n","# model.add(GlobalAveragePooling3D())\n","# model.add(Dense(512, activation='relu'))\n","# model.add(BatchNormalization())\n","# model.add(Dense(5, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"v5NMALbtpV_8"},"source":["Number of Epochs: 40\u003cbr /\u003e\n","Training Accuracy: 0.99 | Validation Accuracy: 0.91 \u003cbr /\u003e\n","While both training and validation accuracy are in the 0.9 range, the training accuracy's proximity to 1 suggests potential overfitting. \u003cbr /\u003e\n","Considering the likelihood of overfitting, experimenting with different model architectures is advised."]},{"cell_type":"markdown","metadata":{"id":"NrugR9xkpV_8"},"source":["### Model - 8"]},{"cell_type":"code","execution_count":32,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1693191613281,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"zWENmlR_pV_8"},"outputs":[],"source":["# # model - 8 Implementing a CNN with LSTM architecture, while decreasing the number of epochs to 20.\n","\n","# model = Sequential()\n","\n","# model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'), input_shape=input_shape))\n","# model.add(TimeDistributed(BatchNormalization()))\n","# model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","\n","# model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n","# model.add(TimeDistributed(BatchNormalization()))\n","# model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","\n","# model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n","# model.add(TimeDistributed(BatchNormalization()))\n","# model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","\n","# model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n","# model.add(TimeDistributed(BatchNormalization()))\n","# model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","\n","# model.add(TimeDistributed(Conv2D(256, (3, 3) , padding='same', activation='relu')))\n","# model.add(TimeDistributed(BatchNormalization()))\n","# model.add(TimeDistributed(MaxPooling2D((2, 2))))\n","\n","# model.add(TimeDistributed(Flatten()))\n","# model.add(LSTM(64))\n","# model.add(Dropout(0.25))\n","\n","# model.add(Dense(64,activation='relu'))\n","# model.add(Dropout(0.25))\n","\n","# model.add(Dense(5, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"nNxRHsZqpV_9"},"source":["Number of Epochs: 20 \u003cbr /\u003e\n","Training Accuracy: 0.50 | Validation Accuracy: 0.66 \u003cbr /\u003e\n","The model's accuracy falls below expectations. Exploring an alternate architecture: Conv2D with GRU."]},{"cell_type":"markdown","metadata":{"id":"vmhDCPj6pV_-"},"source":["### Model - 9"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":0,"status":"ok","timestamp":1693191613621,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"zTQD44pEpWAL"},"outputs":[],"source":["# # model - 9 Implementing a TimeDistributed Conv2D combined with GRU architecture.\n","\n","# model = Sequential()\n","\n","# model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape))\n","# model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","# model.add(BatchNormalization())\n","\n","# model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n","# model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","# model.add(BatchNormalization())\n","\n","# model.add(TimeDistributed(GlobalAveragePooling2D()))\n","# model.add(TimeDistributed(Dense(63, activation='relu')))\n","# model.add(BatchNormalization())\n","\n","# model.add(GRU(128))\n","# model.add(BatchNormalization())\n","# model.add(Dense(5, activation='relu'))"]},{"cell_type":"markdown","metadata":{"id":"n2ZooZojpWAL"},"source":["Number of Epochs: 40 \u003cbr /\u003e\n","Training Accuracy: 0.96 | Validation Accuracy: 0.81 \u003cbr /\u003e\n","The notable gap between training and validation accuracy indicates potential overfitting. Incorporating Dropout layers into the model to address this."]},{"cell_type":"markdown","metadata":{"id":"KDJbtqCnpWAL"},"source":["### Model - 10"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":475,"status":"ok","timestamp":1693191614430,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"75Na69rYpWAM"},"outputs":[],"source":["# model - 10 # Implementing a TimeDistributed Conv2D combined with GRU architecture.\n","\n","model = Sequential()\n","\n","model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape))\n","model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","model.add(BatchNormalization())\n","\n","model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n","model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","model.add(BatchNormalization())\n","\n","model.add(TimeDistributed(GlobalAveragePooling2D()))\n","model.add(TimeDistributed(Dense(63, activation='relu')))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.2))\n","\n","model.add(GRU(128))\n","model.add(BatchNormalization())\n","model.add(Dense(5, activation='relu'))"]},{"cell_type":"markdown","metadata":{"id":"oz1z-Ej0pWAM"},"source":["Number of Epochs: 40 \u003cbr /\u003e\n","Training Accuracy: 0.86 | Validation Accuracy: 0.58 \u003cbr /\u003e\n","Shifting the model to TimeDistributed Conv2D with GlobalAveragePooling3D, along with extending the number of epochs to 50."]},{"cell_type":"markdown","metadata":{"id":"yEjwAAsFpWAM"},"source":["### Model - 11"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":507,"status":"ok","timestamp":1693191615651,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"VqlWIoswpWAM"},"outputs":[],"source":["\n","# model - 11 Implementing a TimeDistributed Conv2D model with GlobalAveragePooling3D.\n","\n","model = Sequential()\n","\n","model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape))\n","model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","model.add(BatchNormalization())\n","\n","model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n","model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","model.add(BatchNormalization())\n","\n","model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n","model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","model.add(BatchNormalization())\n","\n","model.add(GlobalAveragePooling3D())\n","model.add(Dense(256, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(5, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"RNi_IQcHpWAM"},"source":["Epochs: 50 \u003cbr /\u003e\n","Training Accuracy: 0.99 | Validation Accuracy: 0.91 \u003cbr /\u003e\n","Both training and validation accuracy are within the 0.9 range. \u003cbr /\u003e\n","Yet, the proximity of the training accuracy to 1 suggests potential overfitting. \u003cbr /\u003e\n","Incorporating Dropout layers into the model (10), while maintaining the same number of epochs."]},{"cell_type":"markdown","metadata":{"id":"kUuqke9dpWAN"},"source":["### Model - 12"]},{"cell_type":"code","execution_count":36,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693191615652,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"1RF7siiwpWAN"},"outputs":[],"source":["model = Sequential()\n","\n","model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu'), input_shape=input_shape))\n","model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","model.add(BatchNormalization())\n","\n","model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu')))\n","model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(TimeDistributed(Conv2D(128, (3, 3), activation='relu')))\n","model.add(TimeDistributed(MaxPooling2D(2, 2)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.25))\n","\n","model.add(GlobalAveragePooling3D())\n","model.add(Dense(256, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.5))\n","model.add(Dense(5, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"AMLKxFdQpWAN"},"source":["Number of Epochs: 50 \u003cbr /\u003e\n","Training Accuracy: 0.93 | Validation Accuracy: 0.91 \u003cbr /\u003e\n","Both training and validation accuracy are approximately in the 0.9 range. \u003cbr /\u003e\n","Furthermore, the disparity between the accuracies for the training and validation sets is relatively small. \u003cbr /\u003e\n","Opting for **Model - 12** (TimeDistributed Conv2D with GlobalAveragePooling3D and Dropouts)."]},{"cell_type":"markdown","metadata":{"id":"TX2AksRFpWAN"},"source":["The subsequent action involves `compiling` the model. Upon printing the model's `summary`, you'll gain insight into the overall count of parameters requiring training."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1693191616053,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"ZVDb2JZ2pWAN","outputId":"5bbe8504-ca4d-4e29-a063-2a449e58c80a"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"name":"stdout","output_type":"stream","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," time_distributed_12 (TimeDi  (None, 20, 98, 98, 32)   896       \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_13 (TimeDi  (None, 20, 49, 49, 32)   0         \n"," stributed)                                                      \n","                                                                 \n"," batch_normalization_8 (Batc  (None, 20, 49, 49, 32)   128       \n"," hNormalization)                                                 \n","                                                                 \n"," time_distributed_14 (TimeDi  (None, 20, 47, 47, 64)   18496     \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_15 (TimeDi  (None, 20, 23, 23, 64)   0         \n"," stributed)                                                      \n","                                                                 \n"," batch_normalization_9 (Batc  (None, 20, 23, 23, 64)   256       \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_1 (Dropout)         (None, 20, 23, 23, 64)    0         \n","                                                                 \n"," time_distributed_16 (TimeDi  (None, 20, 21, 21, 128)  73856     \n"," stributed)                                                      \n","                                                                 \n"," time_distributed_17 (TimeDi  (None, 20, 10, 10, 128)  0         \n"," stributed)                                                      \n","                                                                 \n"," batch_normalization_10 (Bat  (None, 20, 10, 10, 128)  512       \n"," chNormalization)                                                \n","                                                                 \n"," dropout_2 (Dropout)         (None, 20, 10, 10, 128)   0         \n","                                                                 \n"," global_average_pooling3d_1   (None, 128)              0         \n"," (GlobalAveragePooling3D)                                        \n","                                                                 \n"," dense_10 (Dense)            (None, 256)               33024     \n","                                                                 \n"," batch_normalization_11 (Bat  (None, 256)              1024      \n"," chNormalization)                                                \n","                                                                 \n"," dropout_3 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_11 (Dense)            (None, 5)                 1285      \n","                                                                 \n","=================================================================\n","Total params: 129,477\n","Trainable params: 128,517\n","Non-trainable params: 960\n","_________________________________________________________________\n","None\n"]}],"source":["optimiser = optimizers.Adam(lr=0.01)  # write your optimizer\n","model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{"id":"NlGCWCQEpWAO"},"source":["We will now generate the `train_generator` and `val_generator`, both of which will be employed in the `.fit_generator` function."]},{"cell_type":"code","execution_count":38,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693191617607,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"KuEGQq99pWAO"},"outputs":[],"source":["# Create data generators for training and validation\n","train_generator = generator(train_path, train_doc, batch_size)\n","val_generator = generator(val_path, val_doc, batch_size)"]},{"cell_type":"code","execution_count":39,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1693191618450,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"UZf6u2BCpWAP"},"outputs":[],"source":["# Model Checkpoints and Callbacks\n","model_name = 'model_init_{}/'.format(str(curr_dt_time).replace(' ', '').replace(':', '_'))\n","\n","if not os.path.exists(model_name):\n","    os.mkdir(model_name)\n","\n","filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n","\n","checkpoint = ModelCheckpoint(filepath,\n","                             monitor='val_loss',\n","                             verbose=1,\n","                             save_best_only=False,\n","                             save_weights_only=False,\n","                             mode='auto',\n","                             save_freq='epoch')\n","\n","LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001, verbose=1)  # write the REducelronplateau code here\n","\n","callbacks_list = [checkpoint, LR]"]},{"cell_type":"markdown","metadata":{"id":"RUlmAXXupWAP"},"source":["The `steps_per_epoch` and `validation_steps` parameters are utilized by the `fit` method to determine the count of `next()` calls required."]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":347,"status":"ok","timestamp":1693191621087,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"SlD0rqkspWAQ"},"outputs":[],"source":["# Determining Steps per Epoch and Validation Steps\n","if (num_train_sequences%batch_size) == 0:\n","    steps_per_epoch = int(num_train_sequences/batch_size)\n","else:\n","    steps_per_epoch = (num_train_sequences//batch_size) + 1\n","\n","if (num_val_sequences%batch_size) == 0:\n","    validation_steps = int(num_val_sequences/batch_size)\n","else:\n","    validation_steps = (num_val_sequences//batch_size) + 1"]},{"cell_type":"markdown","metadata":{"id":"y53t4w-ZpWAQ"},"source":["We will proceed to fit the model, initiating the training process. The checkpoints will facilitate the saving of the model at the conclusion of every epoch."]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1693191623136,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"Vis1JeE5pWAQ","outputId":"0d67eef0-c58e-468c-a358-bc8b23c24bf1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epochs: 50\n"]}],"source":["# Number of Epochs\n","num_epochs =50\n","print(f\"Epochs: {num_epochs}\")\n","# Training the Model"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":13888756,"status":"error","timestamp":1693205513071,"user":{"displayName":"Joseph Hanna","userId":"04296596482967779965"},"user_tz":-240},"id":"ZQCn0ki_pWAR"},"outputs":[{"name":"stdout","output_type":"stream","text":["Source Path: /content/drive/My Drive/Gesture Recognition/Project_data/Project_data/train; Batch Size: 39\n"]},{"name":"stderr","output_type":"stream","text":["\u003cipython-input-6-1aa380e1af7f\u003e:11: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n","  image = imread('{0}/{1}/{2}'.format(source_path, t[folder + (batch * batch_size)].strip().split(';')[0], imgs[item])).astype(np.float32)\n"]}],"source":["history = model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n","                    callbacks=callbacks_list, validation_data=val_generator,\n","                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gSF0uQofpWAR"},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","# Plotting Model Loss and Accuracy\n","plt.figure(figsize=(20,6))\n","ax1 = plt.subplot(121)\n","ax1 = plt.plot(history.history['loss'])\n","ax1 = plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='lower left')\n","ax2 = plt.subplot(122)\n","ax2 = plt.plot(history.history['categorical_accuracy'])\n","ax2 = plt.plot(history.history['val_categorical_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('categorical_accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='lower left')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"un2oBFPRpWAR"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"waOa-Z8bUrJu"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}